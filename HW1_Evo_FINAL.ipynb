{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThiTOMIH6GEq"
   },
   "source": [
    "##HW 01 - Line Search Optimization\n",
    "***TecnolÃ³gico de Monterrey - MCCi - Evolutionary Computation Course***\n",
    "*   Luis Felipe Brenes - A00819817\n",
    "*   Diego Acosta Ugalde - A01367987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjQPG3siBF0w"
   },
   "source": [
    "In this jupyter notebook we implement three diferent kinds of optimization methods that can be used to maximize or minimize mathematical function:\n",
    "\n",
    "\n",
    "*   Line Search (using gradient descent with Wolfe's Conditions)\n",
    "*   Line Search (using Newton's Method)\n",
    "*   Hill Climber Optimization\n",
    "\n",
    "Then these methods are compared taking into account the following measures:\n",
    "\n",
    "\n",
    "*   **Correctness:** Does it find the global minimum? How big is the error between the returned and the expected output?\n",
    "*   **Efficiency:** Does it find a good enough solution fast? How many iterations does it take for the algorithm to return an estimated minimum?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5gMxoGpC-W-"
   },
   "source": [
    "###**STEP 0: Importing needed modules**\n",
    "*note: sympy is a library that enables python to compute math as symbolic expressions. You can find more about it here: https://www.sympy.org/en/index.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "_kn5tuVosYmW"
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x, y = sp.symbols('x y') #defines x y as the symbols to be used in sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BOMXcdApE_Nd"
   },
   "outputs": [],
   "source": [
    "c_nabla = 0 #Iteration counter. Counts how many times the gradient is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvrIW1KRDPU1"
   },
   "source": [
    "###**STEP 1: Creating needed functions**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5R6wLk_JZWD"
   },
   "source": [
    "####Group 1: Functions for mathematical and vector operations\n",
    "\n",
    "*   **nabla**(function): returns first derivative\n",
    "*   **unit_vect**(vector): returns unit vector\n",
    "*   **prodScalarVect**(scalar,vector): returns the product between a scalar and a vector.\n",
    "*   **sum2vect**(vector1,vector2): returns the sum of vector1 and vector2.\n",
    "*   **amplitud_vect**(vector): returns the magnitude of a vector.\n",
    "*   **evaluate_f**(function, X): returns function evaluated in point X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dcTb9aNHpEhn"
   },
   "outputs": [],
   "source": [
    "def nabla(function: sp.core.add.Add) -> list:\n",
    "    decision_variables = [x,y]\n",
    "\n",
    "    global c_nabla\n",
    "    c_nabla+=1\n",
    "\n",
    "    # diff calculates the derivative.\n",
    "    return [sp.diff(function, decision_variable) for decision_variable in decision_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "srBFwXuCr4kZ"
   },
   "outputs": [],
   "source": [
    "def unitVect(v):\n",
    "  return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "lTn2mGxnu7FP"
   },
   "outputs": [],
   "source": [
    "#Vector operations\n",
    "\n",
    "from operator import add\n",
    "def prodScalarVect(S, V):\n",
    "  return [comp * S for comp in V]\n",
    "\n",
    "def sum2vect(V1,V2):\n",
    "  return list(map(add, V1, V2))\n",
    "\n",
    "def amplitud_vect(vect: list, eval_point:list) -> float:\n",
    "  eval_vect = map(lambda x: x.evalf(subs={'x':eval_point[0], 'y':eval_point[1]}),vect)\n",
    "  pot_eval_vect = map(lambda x: x**2, eval_vect)\n",
    "  amp_vect = math.sqrt(sum(pot_eval_vect))\n",
    "  return amp_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Dkijzx5HD-XO"
   },
   "outputs": [],
   "source": [
    "def extract_symbols_from_function(f: sp.core.add.Add):\n",
    "    \"\"\"\n",
    "        Returns a vector with the symbols of a given function sorted by their name\n",
    "        @param f : function whose symbols will be extracted\n",
    "        @returns a list with the symbols of f sorted by their name\n",
    "    \"\"\"\n",
    "    return sorted(f.free_symbols, key=lambda s: s.name)\n",
    "\n",
    "\n",
    "def evaluate_f(f: sp.core.add.Add, x: list) -> float:\n",
    "    \"\"\"\n",
    "        Evaluate a given function in a given coordinates\n",
    "        @param f : function to be evaluated\n",
    "        @param x : a list to evaluate f in\n",
    "        @return f(x) which is a scalar resulting of evaluating f in x\n",
    "    \"\"\"\n",
    "    # First exctract symbols of the function and sort them by name\n",
    "    decision_variables = extract_symbols_from_function(f)\n",
    "    # Then calculate the function evaluated in requested values\n",
    "    zipped_variables_and_their_values = dict(zip(decision_variables, x))\n",
    "    evaluation = f.evalf(subs=zipped_variables_and_their_values, chop=True)\n",
    "    # Convert the result to float and round it to two decimals\n",
    "    #return round(float(evaluation), 2)\n",
    "    return(float(evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vs9_7tyJgIP"
   },
   "source": [
    "####Group 2: Functions to calculate search direction p\n",
    "\n",
    "*   **gradient_descent**(function): returns search direction p using gradient descent.\n",
    "*   **newton_method**(function): returns search direction p using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZCVRj5zEDrX1"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(objF, eval_p, minmax=[-1,1]):\n",
    "  return unitVect(list(map(lambda f: minmax * evaluate_f(f,eval_p), nabla(objF))))\n",
    "  #return list(map(lambda f: minmax * evaluate_f(f,eval_p), nabla(objF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8zgzQyWMgen_"
   },
   "outputs": [],
   "source": [
    "def newton_method(objF, eval_p, minmax=[-1,1]):\n",
    "    hess = [nabla(comp) for comp in nabla(objF)]\n",
    "\n",
    "    # Evaluar el hessiano\n",
    "    ev_hess = []\n",
    "    for comp in hess:\n",
    "      hess_evaluated = [evaluate_f(e, eval_p) for e in comp]\n",
    "      ev_hess.append(hess_evaluated)\n",
    "\n",
    "    #invertir hessiano\n",
    "    inverseHess = np.linalg.inv(ev_hess)\n",
    "\n",
    "    #evaluar nabla\n",
    "    ev_nabla = [evaluate_f(comp, eval_p) for comp in nabla(objF)]\n",
    "\n",
    "    return(minmax*np.matmul(inverseHess, ev_nabla))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQl4jU0EJ3XV"
   },
   "source": [
    "####Group 3: Functions to calculate step size t.\n",
    "\n",
    "*   **wolfeCondition1**(function, X, p, t, c1): returns bool (True/False) if Wolfe Condition 1 is met.\n",
    "\n",
    "*   **wolfeCondition2**(function, X, p, t, c2): returns bool (True/False) if Wolfe Condition 2 is met.\n",
    "\n",
    "* **stepSize**(function, X, p, t): returns step size t, using Wolfe's Conditions.\n",
    "\n",
    "*note: some literature refers to step size as alpha.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PRybHyXRsdQX"
   },
   "outputs": [],
   "source": [
    "def wolfeCondition1(f, X, p, t, c1):\n",
    "\n",
    "  #ineqL = evaluate_f( f, sum2vect(X,prodScalarVect(t,p)))\n",
    "  ineqL  = f.evalf(subs={'x': sum2vect(X,prodScalarVect(t,p))[0], 'y':sum2vect(X,prodScalarVect(t,p))[1]},chop=True)\n",
    "\n",
    "  #ineqR = evaluate_f(f,X) + c1 * t *  np.matmul([evaluate_f(comp, X) for comp in nabla(f)] ,np.transpose(p))\n",
    "  ineqR = f.evalf(subs={'x': X[0], 'y':X[1]},chop=True)+ + c1 * t *  np.matmul([comp.evalf(subs={'x': X[0], 'y':X[1]}) for comp in nabla(f)] ,np.transpose(p))\n",
    "\n",
    "  return  ineqL > ineqR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6u56Dg7H0SYg"
   },
   "outputs": [],
   "source": [
    "def wolfeCondition2(f, X, p, t, c2):\n",
    "  #ineqL = np.matmul ([evaluate_f(comp, sum2vect(X, prodScalarVect(t,p))) for comp in nabla(f)], np.transpose(p))\n",
    "  ineqL = np.matmul ([comp.evalf(subs={'x': sum2vect(X, prodScalarVect(t,p))[0], 'y':sum2vect(X, prodScalarVect(t,p))[1]},chop=True) for comp in nabla(f)], np.transpose(p))\n",
    "\n",
    "  #ineqR = c2 * np.matmul([evaluate_f(comp, X) for comp in nabla(f)] ,np.transpose(p))\n",
    "  ineqR = c2 * np.matmul([comp.evalf(subs={'x':X[0], 'y': X[1]},chop=True)for comp in nabla(f)] ,np.transpose(p))\n",
    "  return  ineqL < ineqR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CjE545eOB1vp"
   },
   "outputs": [],
   "source": [
    "def stepSize(f, X, p, t):\n",
    "  c1 = 0.0001\n",
    "  c2 = 0.9\n",
    "  flag = True\n",
    "  while flag:\n",
    "    if wolfeCondition1(f, X, p, t, c1):\n",
    "      t/=2\n",
    "      continue\n",
    "    elif wolfeCondition2(f, X, p, t, c2):\n",
    "      t*=2\n",
    "    else:\n",
    "      flag = False\n",
    "  return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9ph-P6TqomL"
   },
   "source": [
    "###**STEP 2.1: Hill Climber Method**\n",
    "This functions is the implementation for Hill Climber Method.\n",
    "\n",
    "*   **ensure_xbest**(x_best, f_obj): returns the best point that the hill climber method could find in the number of iterations given (iter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "L2wlEfoXrJ_q"
   },
   "outputs": [],
   "source": [
    "p = 5 # Open ball radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iLLDME_7rLGp"
   },
   "outputs": [],
   "source": [
    "iter = 2000 # Max number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ltCO6AZSqyuw"
   },
   "outputs": [],
   "source": [
    "def ensure_xbest(x_best, f_ob):\n",
    "  print('Start point:', x_best)\n",
    "  i = 0\n",
    "  i_best = 0\n",
    "  tolerance = .001\n",
    "  while i < iter:\n",
    "    # Generate new x in circle\n",
    "    angle = random.uniform(0, 2*math.pi)\n",
    "    distance = random.uniform(0, p)\n",
    "    x = x_best[0] + distance * math.cos(angle)\n",
    "    y = x_best[1] + distance * math.sin(angle)\n",
    "    x_new = [x,y]\n",
    "    # print('X new: ', x_new)\n",
    "    x_eval = f_ob.evalf(subs={'x': x_new[0], 'y': x_new[1]})\n",
    "    # print('X new:', round(float(x_eval), 2))\n",
    "    x_best_eval = f_ob.evalf(subs={'x': x_best[0], 'y': x_best[1]})\n",
    "    # print('X best:', round(float(x_best_eval)))\n",
    "    if (x_eval < x_best_eval):\n",
    "      # print('X new evaluation:', round(float(x_eval), 2))\n",
    "      if abs(x_best_eval-x_eval) <= tolerance:\n",
    "        # No important improvement\n",
    "        break\n",
    "      x_best = x_new\n",
    "      # print('X new best:', x_best)\n",
    "      i_best += 1\n",
    "    i += 1\n",
    "  # print('--------------------------------------')\n",
    "  # print('\\n\\nNumber of iterations: ', i)\n",
    "  # print('Number of best X found: ', i_best)\n",
    "\n",
    "  return x_best, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7XBZ3T3LMYm"
   },
   "source": [
    "###**STEP 2.2: Line Search Methods**\n",
    "These two functions are the implementation of Line Search Method.\n",
    "\n",
    "One uses gradient descent and Wolfe's Conditions\n",
    "\n",
    "####**line_search_grad**(objF, tol, startX, minmax=[-1,1])\n",
    "\n",
    "* Performs line search on an objective function using gradient descent and Wolfe's Conditions.\n",
    "* Takes as arguments *objF (objective function)*, *tol (tolerance)*, *startX (starting point)*, *minmax (1:maximixes, -1:minimizes)*\n",
    "* Returns estimated minimum or maximum of the objectife function.\n",
    "\n",
    "####**line_search_newt**(objF, tol, startX, minmax=[-1,1])\n",
    "\n",
    "* Performs line search on an objective function using gradient descent and Wolfe's Conditions.\n",
    "* Takes as arguments *objF (objective function)*, *tol (tolerance)*, *startX (starting point)*, *minmax (1:maximixes, -1:minimizes)*\n",
    "* Wolfe's Conditions are not used. **p** is used to calculate the Line Search on it's own.\n",
    "* Returns estimated minimum or maximum of the objectife function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MG2F_hljlO5b"
   },
   "outputs": [],
   "source": [
    "def line_search_grad (objF, tol, startX, minmax=[-1,1]):\n",
    "  X = startX\n",
    "  c = 0 #iteration counter\n",
    "  t = 5 #initial step size\n",
    "\n",
    "  while amplitud_vect(nabla(objF), X) >= tol:\n",
    "    #print(\"X =\", X, \"t= \",t)\n",
    "    p_gd = gradient_descent(objF,X,minmax) #p using gradient descent\n",
    "\n",
    "    t = stepSize(objF, X, p_gd, t) #VAMOS EN ESTE PASO\n",
    "    Xnew = sum2vect(X, prodScalarVect(t,p_gd))\n",
    "    if X == Xnew:\n",
    "      break\n",
    "    X = Xnew\n",
    "    c+=1\n",
    "    if c+c_nabla>= 1000: #HERE YOU CAN CHANGE ITERATION LIMIT\n",
    "      print(\"\\n\\nITERATION LIMIT REACHED\")\n",
    "      return X,c\n",
    "  return X, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "NVRtiC3ayiLG"
   },
   "outputs": [],
   "source": [
    "def line_search_newt (objF, tol, startX, minmax=[-1,1]):\n",
    "  X = startX\n",
    "  c = 0 #iteration counter\n",
    "  t = 1 #initial step size\n",
    "\n",
    "  while amplitud_vect(nabla(objF), X) >= tol:\n",
    "    #print(\"X =\", X, \"t= \",t)\n",
    "    p = newton_method(objF,X,minmax) #p using newton method\n",
    "\n",
    "    #t = stepSize(objF, X, p, t, tol)\n",
    "    Xnew = sum2vect(X, prodScalarVect(t,p))\n",
    "    if X == Xnew:\n",
    "      break\n",
    "    X = Xnew\n",
    "    c+=1\n",
    "    if c+c_nabla>= 1000: #HERE YOU CAN CHANGE ITERATION LIMIT\n",
    "      print(\"\\n\\nITERATION LIMIT REACHED\")\n",
    "      return X,c\n",
    "  return X, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxeJeUzxXTez"
   },
   "source": [
    "###**STEP 4: Performance Evaluation**\n",
    "Evaluates performance of different methods printing: **returned optimal**, **real optimal**, **error** calculated using the 2-norm, and **number of iterations** each method needed to calculate the optimal. The number of iterations takes into account each main function iteration and each gradient and hessian calculated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycyOV5l3r8Su"
   },
   "source": [
    "####**Hill Climber evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biB9BVJEsM3q",
    "outputId": "deb73a91-a077-4358-e0cc-7e24651bf033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start point: [-2, 2]\n",
      "FUNCTION #1\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [1.8736939491821842, 0.9164589136067106]\n",
      "    Real Optimal:  [2, 1]\n",
      "    2-Norm Error:  0.1514\n",
      "      Iterations:  508 \n",
      "\n",
      "\n",
      "Start point: [-1.5, 1.5]\n",
      "FUNCTION #2\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [1.0325208400600356, 1.0518012097895877]\n",
      "    Real Optimal:  [1, 1]\n",
      "    2-Norm Error:  0.0612\n",
      "      Iterations:  2000 \n",
      "\n",
      "\n",
      "Start point: [-2, 2]\n",
      "FUNCTION #3\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [-0.00574771897050937, -0.9975452531588039]\n",
      "    Real Optimal:  [0, 0]\n",
      "    2-Norm Error:  0.9976\n",
      "      Iterations:  2000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = sp.symbols('x y')\n",
    "\n",
    "optimal = [2,1]\n",
    "start_point = [-2,2]\n",
    "F1 = -1*(2*x*y + 2*x - x**2 - 2*y**2)\n",
    "x_best1, c = ensure_xbest(start_point, F1)\n",
    "print('FUNCTION #1')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', x_best1)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(x_best1)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c,'\\n\\n')\n",
    "\n",
    "optimal = [1,1]\n",
    "start_point = [-1.5,1.5]\n",
    "F2 = 100*(x**2 - y)**2 + (1 - x)**2\n",
    "x_best2, c = ensure_xbest(start_point, F2)\n",
    "print('FUNCTION #2')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', x_best2)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(x_best2)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c,'\\n\\n')\n",
    "\n",
    "optimal = [0,0]\n",
    "start_point = [-2,2]\n",
    "F3 = 20 + (x**2 - 10 * sp.cos(2 * sp.pi * x)) + (y**2 - 10 * sp.cos(2 * sp.pi * y))\n",
    "x_best3, c = ensure_xbest(start_point, F3)\n",
    "print('FUNCTION #3')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', x_best3)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(x_best3)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c,'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAjDNf_0YRdG"
   },
   "source": [
    "####**Line Search using Newton's Method and a fixed t value on 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaR3Q0h-MiRD",
    "outputId": "c777ca0f-a323-4668-c242-269b4f6af276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTION #1\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [2.0, 1.0]\n",
      "    Real Optimal:  [2, 1]\n",
      "    2-Norm Error:  0.0\n",
      "      Iterations:  7 \n",
      "\n",
      "\n",
      "FUNCTION #2\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [0.9999997317006529, 0.9999994634013711]\n",
      "    Real Optimal:  [1, 1]\n",
      "    2-Norm Error:  0.0\n",
      "      Iterations:  31 \n",
      "\n",
      "\n",
      "FUNCTION #3\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [-1.9899122337085495, 2.0100877662914503]\n",
      "    Real Optimal:  [0, 0]\n",
      "    2-Norm Error:  2.8285\n",
      "      Iterations:  23 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_nabla = 0\n",
    "F1 = -2*x*y - 2*x + x**2 + 2*y**2 # minimo deberÃ­a ser [2,1]\n",
    "tol1 = 0.1\n",
    "X1 = [-2,2]\n",
    "optimal = [2,1]\n",
    "ls_newton1, c = line_search_newt(F1, tol1, X1, -1)\n",
    "\n",
    "print('FUNCTION #1')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', ls_newton1)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(ls_newton1)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c+c_nabla,'\\n\\n')\n",
    "\n",
    "\n",
    "c_nabla = 0\n",
    "F2 = 100 * (x**2-y)**2 + (1-x)**2 # minimo deberÃ­a ser [1,1]\n",
    "tol2 = 0.1\n",
    "X2 = [-1.5,1.5]\n",
    "optimal = [1,1]\n",
    "ls_newton2, c = line_search_newt(F2, tol2, X2, -1)\n",
    "\n",
    "print('FUNCTION #2')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', ls_newton2)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(ls_newton2)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c+c_nabla,'\\n\\n')\n",
    "\n",
    "c_nabla = 0\n",
    "F3 = 20 + (x**2 - 10 * sp.cos(2 * sp.pi * x)) + (y**2 - 10 * sp.cos(2 * sp.pi * y)) # mÃ­nimo deberÃ­a ser [0,0]\n",
    "tol3 = 0.1\n",
    "X3 = [-2,2]\n",
    "optimal = [0,0]\n",
    "ls_newton3, c = line_search_newt(F3, tol3, X3, -1)\n",
    "\n",
    "\n",
    "print('FUNCTION #3')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', ls_newton3)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(ls_newton3)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c+c_nabla,'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNfD6aA3YKuo"
   },
   "source": [
    "####**Line Search using Gradient Descent and Wolfe's Conditions**\n",
    "Iteration Limit is set to 30000 on reported results. It takes around 5-10min to run this cell. You can reduce iteration limit altering that line of code **line_search_grad**() and **line_search_newt**().\n",
    "\n",
    "Iteration limit is currently set to **1000** in order to make it return a result faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1DriD8tRcP7",
    "outputId": "dbbc38eb-35d4-4158-c63d-28727ff1ae26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTION #1\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [1.9334044105373784, 0.97465703599302]\n",
      "    Real Optimal:  [2, 1]\n",
      "    2-Norm Error:  0.0713\n",
      "      Iterations:  56 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ITERATION LIMIT REACHED\n",
      "FUNCTION #2\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [-1.0856085567251808, 1.1754716212949166]\n",
      "    Real Optimal:  [1, 1]\n",
      "    2-Norm Error:  2.093\n",
      "      Iterations:  1005 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ITERATION LIMIT REACHED\n",
      "FUNCTION #3\n",
      "------------------------------------------------------------\n",
      "Returned Optimal:  [-1.999999990481965, 2.000000009518]\n",
      "    Real Optimal:  [0, 0]\n",
      "    2-Norm Error:  2.8284\n",
      "      Iterations:  1002 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_nabla = 0\n",
    "F1 = -2*x*y - 2*x + x**2 + 2*y**2 # minimo deberÃ­a ser [2,1]\n",
    "tol1 = 0.1\n",
    "X1 = [-2,2]\n",
    "optimal = [2,1]\n",
    "ls_grad1, c = line_search_grad(F1, tol1, X1, -1)\n",
    "\n",
    "print('FUNCTION #1')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', ls_grad1)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(ls_grad1)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c+c_nabla,'\\n\\n')\n",
    "\n",
    "\n",
    "c_nabla = 0\n",
    "F2 = 100 * (x**2-y)**2 + (1-x)**2 # minimo deberÃ­a ser [1,1]\n",
    "tol2 = 0.1\n",
    "X2 = [-1.5,1.5]\n",
    "optimal = [1,1]\n",
    "ls_grad2, c = line_search_grad(F2, tol2, X2, -1)\n",
    "\n",
    "print('FUNCTION #2')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', ls_grad2)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(ls_grad2)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c+c_nabla,'\\n\\n')\n",
    "\n",
    "c_nabla = 0\n",
    "#F3 = 20 + (x*2 - 10 * sp.cos(2 * sp.pi * x)) + (y*2 - 10 * sp.cos(2 * sp.pi * y)) # mÃ­nimo deberÃ­a ser [0,0]\n",
    "F3 = 20 + (x**2 - 10 * sp.cos(2 * sp.pi * x)) + (y**2 - 10 * sp.cos(2 * sp.pi * y))\n",
    "tol3 = 0.1\n",
    "X3 = [-2,2]\n",
    "optimal = [0,0]\n",
    "ls_grad3, c = line_search_grad(F3, tol3, X3, -1)\n",
    "\n",
    "\n",
    "print('FUNCTION #3')\n",
    "print('------------------------------------------------------------')\n",
    "print('Returned Optimal: ', ls_grad3)\n",
    "print('    Real Optimal: ', optimal)\n",
    "print('    2-Norm Error: ', round(np.linalg.norm(np.array(ls_grad3)-np.array(optimal)),4))\n",
    "print('      Iterations: ', c+c_nabla,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15.892969131469727 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Time: %s seconds' %(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "T5gMxoGpC-W-",
    "DvrIW1KRDPU1",
    "c5R6wLk_JZWD",
    "4vs9_7tyJgIP",
    "J9ph-P6TqomL",
    "k7XBZ3T3LMYm",
    "ycyOV5l3r8Su",
    "BAjDNf_0YRdG",
    "tNfD6aA3YKuo",
    "9zjdAu4c7p39"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
